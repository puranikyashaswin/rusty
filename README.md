# ğŸ¦€ Rusty ML Framework

> **GPU-Accelerated Machine Learning in Pure Rust**

[![Rust](https://img.shields.io/badge/rust-1.75+-orange.svg)](https://www.rust-lang.org)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Metal](https://img.shields.io/badge/backend-Metal-purple.svg)](https://developer.apple.com/metal/)
[![WebGPU](https://img.shields.io/badge/backend-WebGPU-green.svg)](https://www.w3.org/TR/webgpu/)

Rusty is a high-performance ML framework written entirely in Rust, designed for training and fine-tuning Large Language Models on consumer hardware. It achieves **120+ GFLOPS** on Apple M2 with custom WGSL compute shaders.

<p align="center">
  <img src="docs/benchmark.png" alt="Benchmark Results" width="600">
</p>

## âœ¨ Features

- ğŸš€ **GPU Acceleration** - Native Metal/WebGPU backend via wgpu
- ğŸ§  **Flash Attention** - O(N) memory instead of O(NÂ²)
- ğŸ¯ **LoRA Fine-tuning** - Train billion-parameter models on 8GB RAM
- âš¡ **Mixed Precision** - FP16 training with dynamic loss scaling
- ğŸ“¦ **HuggingFace Compatible** - Load any LLaMA/Mistral/Phi model
- ğŸ”¥ **Custom WGSL Kernels** - Hand-optimized compute shaders (1000+ lines)

## ğŸ“Š Performance

Tested on Apple M2 (Metal backend):

| Operation | Size | Performance |
|-----------|------|-------------|
| MatMul | 4096Â³ | **121 GFLOPS** |
| MatMul | 2048Â³ | 114 GFLOPS |
| MatMul | 1024Â³ | 109 GFLOPS |
| Softmax | 2048Ã—2048 | 850 M elem/s |
| RMSNorm | 4096Ã—2048 | 920 M elem/s |

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/puranikyashaswin/rusty.git
cd rusty

# Build in release mode
cargo build --release
```

### Run Demo

```bash
# Beautiful visual demo (great for videos!)
cargo run -p rusty-cli --release -- --demo
```

### Run Benchmarks

```bash
# GPU performance benchmarks
TMPDIR=/tmp cargo run -p rusty-benchmarks --release
```

### Fine-tune a Model

```bash
# Download a model first
pip install huggingface-hub
huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0

# Fine-tune with LoRA
cargo run -p rusty-cli --release -- ~/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/*/
```

## ğŸ“ Project Structure

```
rusty/
â”œâ”€â”€ rusty-backend/     # GPU compute engine (wgpu + WGSL kernels)
â”œâ”€â”€ rusty-autograd/    # Automatic differentiation
â”œâ”€â”€ rusty-graph/       # Neural network layers (Attention, MLP, etc.)
â”œâ”€â”€ rusty-loader/      # SafeTensors + Tokenizer loading
â”œâ”€â”€ rusty-trainer/     # Training loop + GradScaler
â”œâ”€â”€ rusty-hub/         # HuggingFace model integration
â”œâ”€â”€ rusty-cli/         # Command-line interface
â””â”€â”€ benchmarks/        # Performance benchmarks
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         rusty-cli                               â”‚
â”‚                    (User Interface Layer)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    rusty-trainer    â”‚    rusty-hub    â”‚    rusty-loader         â”‚
â”‚   (Training Loop)   â”‚ (Model Loading) â”‚  (SafeTensors/Tokenizer)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       rusty-graph                               â”‚
â”‚     (Neural Networks: Attention, MLP, Flash Attention)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      rusty-autograd                             â”‚
â”‚        (Automatic Differentiation + Optimizers)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      rusty-backend                              â”‚
â”‚    (GPU Compute: wgpu + 1000+ lines of WGSL kernels)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Metal/WebGPU  â”‚
                    â”‚   (Apple M1/M2) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Supported Models

| Model Family | Status | Notes |
|--------------|--------|-------|
| LLaMA / LLaMA-2 / LLaMA-3 | âœ… | Full support |
| Mistral / Mixtral | âœ… | Sliding window attention |
| TinyLlama | âœ… | Great for testing |
| Phi / Phi-2 / Phi-3 | âœ… | Microsoft models |
| Qwen / Qwen-2 | âœ… | Alibaba models |
| Gemma / Gemma-2 | âœ… | Google models |

## ğŸ“– Usage Examples

### Basic Inference

```rust
use rusty_backend::{WgpuContext, ComputeEngine};
use rusty_graph::FlashAttention;

#[tokio::main]
async fn main() {
    // Initialize GPU
    let ctx = WgpuContext::new().await;
    let engine = ComputeEngine::new(&ctx);
    
    // Create Flash Attention layer
    let attn = FlashAttention::new(768, 12, true);
    println!("Memory savings: {}", attn.memory_savings(2048));
}
```

### Training with LoRA

```rust
use rusty_trainer::{GradScaler, DemoConfig, run_demo};

fn main() {
    // Run demo with default config
    run_demo(DemoConfig {
        model_name: "TinyLlama-1.1B".to_string(),
        batch_size: 4,
        learning_rate: 1e-4,
        num_epochs: 3,
        use_fp16: true,
        use_lora: true,
        lora_rank: 8,
        max_seq_len: 512,
    });
}
```

### Custom Dataset

Create a JSON file:
```json
[
  {"prompt": "What is Rust?", "response": "Rust is a systems programming language."},
  {"prompt": "Explain ML", "response": "Machine learning enables computers to learn from data."}
]
```

Then train:
```bash
cargo run -p rusty-cli --release -- /path/to/model ./data/custom.json
```

## ğŸ› ï¸ Development

### Prerequisites

- Rust 1.75+ (`rustup update`)
- macOS with Apple Silicon (M1/M2/M3) OR
- Any platform with Vulkan support

### Building

```bash
# Debug build
cargo build

# Release build (optimized)
cargo build --release

# Run tests
cargo test --workspace
```

### Running Benchmarks

```bash
# Full benchmark suite
TMPDIR=/tmp cargo run -p rusty-benchmarks --release

# Quick check
cargo check --workspace
```

## ğŸ“ˆ Roadmap

- [x] GPU Backend (Metal/WebGPU)
- [x] Custom WGSL Kernels (MatMul, Softmax, RMSNorm, SiLU)
- [x] Flash Attention (Forward + Backward)
- [x] LoRA Fine-tuning
- [x] Mixed Precision Training (FP16)
- [x] HuggingFace Model Loading
- [x] Benchmarks
- [ ] CUDA Backend
- [ ] Quantization (INT4/INT8)
- [ ] Distributed Training
- [ ] ONNX Export

## ğŸ¤ Contributing

Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) first.

```bash
# Fork the repo
# Create a branch
git checkout -b feature/amazing-feature

# Make changes and test
cargo test --workspace

# Commit and push
git commit -m "feat: add amazing feature"
git push origin feature/amazing-feature

# Open a Pull Request
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

- [wgpu](https://github.com/gfx-rs/wgpu) - Cross-platform GPU API
- [safetensors](https://github.com/huggingface/safetensors) - Safe tensor serialization
- [Flash Attention](https://arxiv.org/abs/2205.14135) - Memory-efficient attention

---

<p align="center">
  <b>Built with ğŸ¦€ Rust + âš¡ GPU Power</b>
</p>
